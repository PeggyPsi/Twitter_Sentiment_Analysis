{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the libraries we need for later use\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "# import sklearn as sk\n",
    "# uncoment the next line the first time you runthis.\n",
    "# nltk.download('all') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ....................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD DATA (Train File and Test File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train dataset\n",
    "def load_data(file, col_names, n=0):\n",
    "    #Read all data\n",
    "    if n==0:\n",
    "        data = pd.read_csv(file, sep=\"\\t\", header=None, names=col_names, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "    #Read specific number of rows of data\n",
    "    else:\n",
    "        data = pd.read_csv(file, nrows=n, sep=\"\\t\", header=None, names=col_names, quoting=csv.QUOTE_NONE, error_bad_lines=False)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve rows of data and tweets for further exploitation\n",
    "#Train file\n",
    "train_file = '../twitter_data/train2017.tsv'\n",
    "col_names = ['TweetID', 'ID', 'Sentiment', 'Tweet']\n",
    "train_data = load_data(train_file, col_names)\n",
    "train_data = train_data[['TweetID', 'Sentiment', 'Tweet']]\n",
    "# train_data = train_data[:50]\n",
    "\n",
    "#Test file\n",
    "test_file = '../twitter_data/test2017.tsv'\n",
    "col_names = ['TweetID', 'ID', 'Sentiment', 'Tweet']\n",
    "test_data = load_data(test_file, col_names)\n",
    "test_data = test_data[['TweetID', 'Tweet']]\n",
    "# test_data = test_data[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lexica\n",
    "valence_tweet = load_data(\"../lexica/emotweet/valence_tweet.txt\", [\"WORD\",\"VALENCE\"])\n",
    "generic = load_data(\"../lexica/generic/generic.txt\", [\"WORD\",\"VALENCE\"])\n",
    "affin = load_data(\"../lexica/affin/affin.txt\", [\"WORD\",\"VALENCE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ....................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE-PROCESSING / DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe that contains further information about extra features\n",
    "cols = ['numof_posemo', 'numof_negemo', 'good_similarity', 'bad_similarity', 'tomorrow_similarity',\n",
    "       'tweet_length', 'sum_valence_1', 'sum_valence_2', 'sum_valence_3', 'max_valence', 'min_valence']\n",
    "\n",
    "train_extra_features = pd.DataFrame(data=0.0, columns=cols, index=np.arange(0, train_data.shape[0]))\n",
    "test_extra_features = pd.DataFrame(data=0.0, columns=cols, index=np.arange(0, train_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOKENIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda x: x.split())\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOWERCASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: [word.lower() for word in word_list])\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: [word.lower() for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REPLACE SPECIAL WORDS (@obama, urls, #hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Replace_Special(tweet):  \n",
    "    for i, word in enumerate(tweet):\n",
    "        if word[0] in ['@', '$', '#'] or any(char.isdigit() for char in word) or \"http://\" in word:\n",
    "            tweet[i] = ''\n",
    "    return tweet\n",
    "   \n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda tweet: Replace_Special(tweet))\n",
    "# train_data['Tweet'].apply(lambda x: print(x))\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda tweet: Replace_Special(tweet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE PUNCTUATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCLUDE EMOJIS\n",
    "positive_emojis = [\":)\", \":D\", \";)\", \":-)\", \"<3\", \"(:\", \":P\", \"XD\", \":p\", \"^^\", \n",
    "                   \";D\", \"o.o\", \":O\", \";s\", \"=)\", \";-)\", \":)))\", \":3\", \":')\", \"\\m/\", \n",
    "                   \"(;\", \"^_^\", \":o\", \"n.o\", \"o-o\", \"<333\", \"^.^\", \":-d\", \"d:\", \":s\",\n",
    "                   \":v\", \":]\", \";o\", \";))\", \":ddd\", \"=)))\", \"^^'\"]\n",
    "negative_emojis = [ \":(\",  \"):\",  \":/\",  \"]:\", \":'(\",  \":-(\",  \":(((\",  \"-___-\", \":-/\", \"/:\", \"-__-\", \n",
    "                  \":((\",  \"._.\", \":|\",  \">.>\", \"(-.-)\",  \":-(((\", \">_<\",  \":,(\", \">:)\", \":\\\\\" ]\n",
    "emojis = positive_emojis + negative_emojis\n",
    "\n",
    "def Remove_Punctuations(s):\n",
    "    if s not in emojis:\n",
    "        return s.translate(str.maketrans('', '', string.punctuation))\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "# Without seperating emojis\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: [Remove_Punctuations(word) for word in word_list])\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: list(filter(None, word_list)))\n",
    "# train_data['Tweet'].apply(lambda x: print(x))\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: [Remove_Punctuations(word) for word in word_list])\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: list(filter(None, word_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find positive and negative emojis\n",
    "def Find_Emojis(data, extra_features):\n",
    "    for index, row in data.iterrows():\n",
    "        tweet = row['Tweet']\n",
    "        for word in tweet:\n",
    "            if word in positive_emojis:\n",
    "                extra_features.at[index, 'numof_posemo'] += 1\n",
    "            elif word in negative_emojis:\n",
    "                extra_features.at[index, 'numof_negemo'] += 1\n",
    "\n",
    "Find_Emojis(train_data, train_extra_features)\n",
    "Find_Emojis(test_data, test_extra_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEMMING / LEMMETIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "word_stemmer = nltk.SnowballStemmer(\"english\")\n",
    "\n",
    "# Lemmatize with POS Tag\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": nltk.corpus.wordnet.ADJ,\n",
    "                \"N\": nltk.corpus.wordnet.NOUN,\n",
    "                \"V\": nltk.corpus.wordnet.VERB,\n",
    "                \"R\": nltk.corpus.wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, nltk.corpus.wordnet.NOUN)\n",
    "\n",
    "def Lemmatize_Stem(word):\n",
    "    word = word_stemmer.stem(word)\n",
    "#     word = word_lemmatizer.lemmatize(word, get_wordnet_pos(word))\n",
    "    return word\n",
    "\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: [Lemmatize_Stem(word) if word not in emojis else word for word in word_list])\n",
    "# train_data['Tweet'].apply(lambda x: print(x))\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: [Lemmatize_Stem(word) if word not in emojis else word for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPELLING CORRECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://rustyonrampage.github.io/text-mining/2017/11/28/spelling-correction-with-python-and-nltk.html\n",
    "\n",
    "def Spelling_Correction(word):\n",
    "    # Reduce lengthening of word\n",
    "    word = re.compile(r\"(.)\\1{2,}\").sub(r\"\\1\\1\", word)\n",
    "    Remove_Punctuations(word) # '\\\\\\\\at'\n",
    "    return word\n",
    "\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: [Spelling_Correction(word) if word not in emojis else word for word in word_list])\n",
    "# train_data['Tweet'].apply(lambda x: print(x))\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: [Spelling_Correction(word) if word not in emojis else word for word in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REMOVE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://www.ranks.nl/stopwords\n",
    "# 153\n",
    "stop_words = { \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"im\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" }\n",
    "# 203\n",
    "stop_words = stop_words.union(nltk.corpus.stopwords.words('english'))\n",
    "# 227\n",
    "stop_words = stop_words.union(set(string.ascii_lowercase))\n",
    "\n",
    "def Remove_Stopwords(word_list):\n",
    "    word_list = [word for word in word_list if word not in stop_words]\n",
    "    return word_list\n",
    "\n",
    "train_data['Tweet'] = train_data['Tweet'].apply(lambda word_list: Remove_Stopwords(word_list))\n",
    "# train_data['Tweet'].apply(lambda x: print(x))\n",
    "test_data['Tweet'] = test_data['Tweet'].apply(lambda word_list: Remove_Stopwords(word_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ....................................................................................................................................................................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create general lexica\n",
    "lexico1 = dict()\n",
    "lexico2 = dict()\n",
    "lexico3 = dict()\n",
    "for index, row in valence_tweet.iterrows():\n",
    "    lexico1.update( {row[\"WORD\"]:row[\"VALENCE\"]})\n",
    "for index, row in generic.iterrows():\n",
    "    lexico2.update( {row[\"WORD\"]:row[\"VALENCE\"]})\n",
    "for index, row in affin.iterrows():\n",
    "    lexico3.update( {row[\"WORD\"]:row[\"VALENCE\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extra features for each tweet\n",
    "def Add_Features(data, extra_features):\n",
    "    # traverse through words of tweet\n",
    "    for index, row in data.iterrows():\n",
    "        tweet = row['Tweet']\n",
    "        value1 = 0\n",
    "        value2 = 0\n",
    "        value3 = 0\n",
    "        max1 = max2 = max3 = -2\n",
    "        min1 = min2 = min3 = 2\n",
    "        for word in tweet:\n",
    "            if word in lexico1.keys(): \n",
    "                value1 = value1 + lexico1[word] \n",
    "                if lexico1[word] > max1:\n",
    "                    max1 = lexico1[word]\n",
    "                if lexico1[word] < min1:\n",
    "                    min1 = lexico1[word]       \n",
    "            if word in lexico2.keys(): \n",
    "                value2 = value2 + lexico2[word]\n",
    "                if lexico2[word] > max2:\n",
    "                    max2 = lexico2[word]\n",
    "                if lexico2[word] < min2:\n",
    "                    min2 = lexico2[word] \n",
    "            if word in lexico3.keys(): \n",
    "                value3 = value3 + lexico3[word]\n",
    "                if lexico3[word] > max3:\n",
    "                    max3 = lexico3[word]\n",
    "                if lexico3[word] < min3:\n",
    "                    min3 = lexico3[word] \n",
    "\n",
    "        # UPDATE\n",
    "        extra_features.at[index, 'tweet_length'] = len(tweet)\n",
    "        extra_features.at[index, 'sum_valence_1'] = value1\n",
    "        extra_features.at[index, 'sum_valence_2'] = value2\n",
    "        extra_features.at[index, 'sum_valence_3'] = value3\n",
    "        extra_features.at[index, 'max_valence'] = max(max1, max2, max3)\n",
    "        extra_features.at[index, 'min_valence'] = min(min1, min2, min3)\n",
    "    return extra_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_extra_features = Add_Features(train_data, train_extra_features)\n",
    "test_extra_features = Add_Features(test_data, test_extra_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows that contain tweets which ended up empty\n",
    "train_data = train_data[train_data.Tweet.astype(bool)]\n",
    "test_data = test_data[test_data.Tweet.astype(bool)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STORE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results in a .pickle file for later use\n",
    "# Tweets are stored in list tokenized form\n",
    "train_data.to_pickle(\"./train_tweets_cleaned.pkl\")\n",
    "test_data.to_pickle(\"./test_tweets_cleaned.pkl\")\n",
    "\n",
    "# Store Extra features\n",
    "train_extra_features.to_pickle(\"./train_extra.pkl\")\n",
    "test_extra_features.to_pickle(\"./test_extra.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
